{"cells":[{"cell_type":"markdown","metadata":{},"source":["# [Conversational AI ChatBot](https://www.kaggle.com/rajkumarl/conversational-ai-chatbot)\n","\n","### Intelligent ChatBot built with Microsoft's DialoGPT transformer to make conversations with human users! \n","\n","![cover image](https://raw.githubusercontent.com/RajkumarGalaxy/dataset/master/Images/robo%20girl.jpg)\n","> ##### [Image by Andy Kelly](https://unsplash.com/@askkell) "]},{"cell_type":"markdown","metadata":{},"source":["### What is a chatbot?\n","\n",">##### A ChatBot is a kind of virtual assistant that can build conversations with human users! A *Chat*ting Ro*bot*. Building a chatbot is one of the popular tasks in Natural Language Processing.\n","\n","### Are all chatbots the same?\n",">##### Chatbots fall under three common categories:\n",">##### 1. Rule-based chatbots\n",">##### 2. Retrieval-based chatbots\n",">##### 3. Intelligent chatbots\n","\n","### Rule-based chatbots\n",">##### These bots respond to users' inputs based on certain pre-specified rules. For instance, these rules can be defined as if-elif-else statements. While writing rules for these chatbots, it is important to expect all possible user inputs, else the bot may fail to answer properly. Hence, rule-based chatbots do not possess any cognitive skills.\n","\n","### Retrieval-based chatbots\n",">##### These bots respond to users' inputs by retrieving the most relevant information from the given text document. The most relevant information can be determined by Natural Language Processing with a scoring system such as cosine-similarity-score. Though these bots use NLP to do conversations, they lack cognitive skills to match a real human chatting companion.\n","\n","### Intelligent AI chatbots\n",">##### These bots respond to users' inputs after understanding the inputs, as humans do. These bots are trained with a Machine Learning Model on a large training dataset of human conversations. These bots are cognitive to match a human in conversing. Amazon's Alexa, Apple's Siri fall under this category. Further, most of these bots can make conversations based on the preceding chat texts.\n","\n","### In this Article?\n",">##### This article describes building an intelligent AI chatbot based on the famous transformer architecture - Microsoft's DialoGPT. According to [Hugging Face's model card](https://huggingface.co/microsoft/DialoGPT-medium), DialoGPT is a State-Of-The-Art large-scale pretrained dialogue response generation model for multiturn conversations. The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Let's Python\n","\n","##### Import necessary libraries and frameworks"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-12-27T03:20:21.345452Z","iopub.status.busy":"2021-12-27T03:20:21.344611Z","iopub.status.idle":"2021-12-27T03:20:29.382776Z","shell.execute_reply":"2021-12-27T03:20:29.382084Z","shell.execute_reply.started":"2021-12-27T03:20:21.345354Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import time\n","import os\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["# Download Microsoft's DialoGPT model and tokenizer\n","\n","##### The Hugging Face checkpoint for the model and its tokenizer is `\"microsoft/DialoGPT-medium\"`"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-12-27T03:20:29.384850Z","iopub.status.busy":"2021-12-27T03:20:29.384121Z","iopub.status.idle":"2021-12-27T03:21:05.195561Z","shell.execute_reply":"2021-12-27T03:21:05.194949Z","shell.execute_reply.started":"2021-12-27T03:20:29.384807Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# download and cache pre-trained model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n","File \u001b[1;32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3335\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[0;32m   3333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3334\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 3335\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3337\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   3338\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   3339\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   3340\u001b[0m     \u001b[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   3341\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   3342\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:532\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[0;32m    530\u001b[0m         extra_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m    531\u001b[0m     weights_only_kwarg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m is_torch_greater_or_equal_than_1_13 \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m--> 532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mweights_only_kwarg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:812\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1051\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m deserialized_objects\n\u001b[0;32m   1050\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m deserialized_objects[key]\n\u001b[1;32m-> 1051\u001b[0m \u001b[43mtyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_should_read_directly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1055\u001b[0m     offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell()\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# checkpoint \n","checkpoint = \"microsoft/DialoGPT-medium\"\n","# download and cache tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","# download and cache pre-trained model\n","model = AutoModelForCausalLM.from_pretrained(checkpoint)"]},{"cell_type":"markdown","metadata":{},"source":["# A ChatBot class"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-12-27T03:21:05.197911Z","iopub.status.busy":"2021-12-27T03:21:05.197163Z","iopub.status.idle":"2021-12-27T03:21:05.215424Z","shell.execute_reply":"2021-12-27T03:21:05.214499Z","shell.execute_reply.started":"2021-12-27T03:21:05.197861Z"},"trusted":true},"outputs":[],"source":["# Build a ChatBot class with all necessary modules to make a complete conversation\n","class ChatBot():\n","    # initialize\n","    def __init__(self):\n","        # once chat starts, the history will be stored for chat continuity\n","        self.chat_history_ids = None\n","        # make input ids global to use them anywhere within the object\n","        self.bot_input_ids = None\n","        # a flag to check whether to end the conversation\n","        self.end_chat = False\n","        # greet while starting\n","        self.welcome()\n","        \n","    def welcome(self):\n","        print(\"Initializing ChatBot ...\")\n","        # some time to get user ready\n","        time.sleep(2)\n","        print('Type \"bye\" or \"quit\" or \"exit\" to end chat \\n')\n","        # give time to read what has been printed\n","        time.sleep(3)\n","        # Greet and introduce\n","        greeting = np.random.choice([\n","            \"Welcome, I am ChatBot, here for your kind service\",\n","            \"Hey, Great day! I am your virtual assistant\",\n","            \"Hello, it's my pleasure meeting you\",\n","            \"Hi, I am a ChatBot. Let's chat!\"\n","        ])\n","        print(\"ChatBot >>  \" + greeting)\n","        \n","    def user_input(self):\n","        # receive input from user\n","        text = input(\"User    >> \")\n","        # end conversation if user wishes so\n","        if text.lower().strip() in ['bye', 'quit', 'exit']:\n","            # turn flag on \n","            self.end_chat=True\n","            # a closing comment\n","            print('ChatBot >>  See you soon! Bye!')\n","            time.sleep(1)\n","            print('\\nQuitting ChatBot ...')\n","        else:\n","            # continue chat, preprocess input text\n","            # encode the new user input, add the eos_token and return a tensor in Pytorch\n","            self.new_user_input_ids = tokenizer.encode(text + tokenizer.eos_token, \\\n","                                                       return_tensors='pt')\n","\n","    def bot_response(self):\n","        # append the new user input tokens to the chat history\n","        # if chat has already begun\n","        if self.chat_history_ids is not None:\n","            self.bot_input_ids = torch.cat([self.chat_history_ids, self.new_user_input_ids], dim=-1) \n","        else:\n","            # if first entry, initialize bot_input_ids\n","            self.bot_input_ids = self.new_user_input_ids\n","        \n","        # define the new chat_history_ids based on the preceding chats\n","        # generated a response while limiting the total chat history to 1000 tokens, \n","        self.chat_history_ids = model.generate(self.bot_input_ids, max_length=1000, \\\n","                                               pad_token_id=tokenizer.eos_token_id)\n","            \n","        # last ouput tokens from bot\n","        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[-1]:][0], \\\n","                               skip_special_tokens=True)\n","        # in case, bot fails to answer\n","        if response == \"\":\n","            response = self.random_response()\n","        # print bot response\n","        print('ChatBot >>  '+ response)\n","        \n","    # in case there is no response from model\n","    def random_response(self):\n","        i = -1\n","        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n","                               skip_special_tokens=True)\n","        # iterate over history backwards to find the last token\n","        while response == '':\n","            i = i-1\n","            response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n","                               skip_special_tokens=True)\n","        # if it is a question, answer suitably\n","        if response.strip() == '?':\n","            reply = np.random.choice([\"I don't know\", \n","                                     \"I am not sure\"])\n","        # not a question? answer suitably\n","        else:\n","            reply = np.random.choice([\"Great\", \n","                                      \"Fine. What's up?\", \n","                                      \"Okay\"\n","                                     ])\n","        return reply"]},{"cell_type":"markdown","metadata":{},"source":["# Happy Chatting!"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-12-27T03:21:05.217343Z","iopub.status.busy":"2021-12-27T03:21:05.217133Z","iopub.status.idle":"2021-12-27T03:31:17.740707Z","shell.execute_reply":"2021-12-27T03:31:17.739565Z","shell.execute_reply.started":"2021-12-27T03:21:05.217317Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing ChatBot ...\n","Type \"bye\" or \"quit\" or \"exit\" to end chat \n","\n","ChatBot >>  Welcome, I am ChatBot, here for your kind service\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  Hi. How are you?\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  I'm good, how are you?\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  I'm fine. Do you cook?\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  I do.\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  What is your favourite recipe?\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  I don't really cook.\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  No problem. What is your favourite food?\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  I don't really eat food.\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  I know. I like sea food a lot.\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  I like that.\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  I have coffee often. How about you?\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  I like coffee.\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  Do you take coffee with milk and sugar?\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  I don't drink coffee.\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  Oh, you like it but don't drink. I understand.\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  I don't drink coffee\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  Shall we have dinner tonight?\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  Fine. What's up?\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  I will book a table and inform you. Be ready.\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  Okay\n"]},{"name":"stdout","output_type":"stream","text":["User    >>  bye\n"]},{"name":"stdout","output_type":"stream","text":["ChatBot >>  See you soon! Bye!\n","\n","Quitting ChatBot ...\n"]}],"source":["# build a ChatBot object\n","bot = ChatBot()\n","# start chatting\n","while True:\n","    # receive user input\n","    bot.user_input()\n","    # check whether to end chat\n","    if bot.end_chat:\n","        break\n","    # output bot response\n","    bot.bot_response()    "]},{"cell_type":"markdown","metadata":{},"source":["# Some sample chats by this ChatBot\n","\n","![chat1](https://raw.githubusercontent.com/RajkumarGalaxy/Conversational-AI-ChatBot/main/chatbot_chats_1.jpg)\n","\n","![chat2](https://raw.githubusercontent.com/RajkumarGalaxy/Conversational-AI-ChatBot/main/chatbot_chats_2.jpg)"]},{"cell_type":"markdown","metadata":{},"source":["##### Thank you for your valuable time!\n","Find this notebook on Kaggle here: https://www.kaggle.com/rajkumarl/conversational-ai-chatbot"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":4}
